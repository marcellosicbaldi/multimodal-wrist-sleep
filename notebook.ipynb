{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data analysis on all the datasets\n",
    "\n",
    "Main Points:\n",
    "- how many nights for each recording?\n",
    "- Is there any non-wear time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import os\n",
    "import glob\n",
    "\n",
    "mpl.rcParams[\"lines.linewidth\"] = 0.96\n",
    "%matplotlib qt\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nonwear import nimbaldetach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hom3ostasis\n",
    "Notes: subject 2_SL has more than 2 days of recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HOM_path = \"/Users/marcellosicbaldi/Library/CloudStorage/OneDrive-AlmaMaterStudiorumUniversitaÌ€diBologna/Serena_Marcello/SleepAnalysis/Datasets/Hom3ostasis/\"\n",
    "HOM_path = \"/Volumes/Untitled/Hom3_CinC/CinC/Data_24hPulses/mod_newQuality\"\n",
    "sub_names = sorted([f for f in os.listdir(HOM_path) if not f.startswith('.')])\n",
    "len(sub_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an input_GGIR and an output_GGIR folder for each subject\n",
    "for i, sub_ID in enumerate(sub_names): \n",
    "    input_GGIR = HOM_path + sub_ID + \"/input_GGIR/\"\n",
    "    output_GGIR = HOM_path + sub_ID + \"/output_GGIR/\"\n",
    "    if not os.path.exists(input_GGIR):\n",
    "        os.makedirs(input_GGIR)\n",
    "    if not os.path.exists(output_GGIR):\n",
    "        os.makedirs(output_GGIR)\n",
    "    print(f\"{i+1}/{len(sub_names)} - {sub_ID} done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save acc data for GGIR and detect non-wear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2_SL\n",
      "Duration: 0 days 16:00:00\n"
     ]
    }
   ],
   "source": [
    "fs_acc = 32\n",
    "sub_names = [\"2_SL\"]\n",
    "for i, sub_ID in enumerate(sub_names):\n",
    "    print(sub_ID)\n",
    "    acc_path = glob.glob(HOM_path + sub_ID + '/**/ACC.csv', recursive=True)[0]\n",
    "    temp_path = glob.glob(HOM_path + sub_ID + '/**/TEMP.csv', recursive=True)[0]\n",
    "\n",
    "    acc_data = pd.read_csv(acc_path, header = None)\n",
    "    acc_start = acc_data.iloc[0,0] # start time in unix\n",
    "    acc = acc_data.drop([0,1]).reset_index(drop=True) / 64 # convert to g\n",
    "    acc.index = pd.date_range(start=pd.to_datetime(acc_start, unit='s'), periods = len(acc), freq = '0.03125s') # timestamps as index\n",
    "    acc.columns = ['x', 'y', 'z']\n",
    "    acc = acc.loc[acc.index[-1] - pd.Timedelta(hours = 16):] # keep only the last 15 hours\n",
    "    # print recording duration\n",
    "    print(f\"Duration: {acc.index[-1] - acc.index[0]}\")\n",
    "    # Save for GGIR\n",
    "    acc.to_csv(HOM_path + sub_ID + '/input_GGIR/acc.csv', index = True, header = True)\n",
    "\n",
    "    ###### TEMP ######\n",
    "    temp_data = pd.read_csv(temp_path, header = None)\n",
    "    temp_start = temp_data.iloc[0,0] \n",
    "    temp = temp_data.iloc[2:].reset_index(drop=True)\n",
    "    temp.index = pd.date_range(start=pd.to_datetime(temp_start, unit='s'), periods = len(temp), freq = '0.25s')\n",
    "    temp.columns = ['temp']\n",
    "    start_stop_nw, _ = nimbaldetach(acc['x'], acc['y'], acc['z'], temp[\"temp\"], accel_freq=32, temperature_freq=4, quiet=True)\n",
    "    start_stop_nw.to_csv(HOM_path + sub_ID + '/input_GGIR/nonwear.csv')\n",
    "\n",
    "    if len(start_stop_nw) > 0:\n",
    "        print(\"Non-wear detected!\")\n",
    "        print(start_stop_nw)\n",
    "        plt.figure(figsize=(10, 6)) \n",
    "        plt.plot(acc[\"x\"].values, label=\"x\")\n",
    "        plt.plot(acc[\"y\"].values, label=\"y\")\n",
    "        plt.plot(acc[\"z\"].values, label=\"z\")\n",
    "        plt.title(sub_ID, fontsize=20)\n",
    "        for _, row in start_stop_nw.iterrows():\n",
    "            start = row[\"Start Datapoint\"]\n",
    "            stop = row[\"End Datapoint\"]\n",
    "            plt.axvspan(start, stop, color='red', alpha=0.5)\n",
    "        plt.legend(loc = \"upper right\")\n",
    "        plt.savefig(HOM_path + sub_ID + '/output_GGIR/nonwear.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb5d95bc130>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(acc.index, acc[\"x\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sleep parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = {sub: [] for sub in sub_names}\n",
    "waso = {sub: [] for sub in sub_names}\n",
    "SE = {sub: [] for sub in sub_names}\n",
    "\n",
    "for i, sub in enumerate(sub_names):\n",
    "    output_GGIR_path = \"/Volumes/Untitled/Hom3_CinC/CinC/Data_24hPulses/mod_newQuality/\" + sub + \"/output_GGIR/output_input_GGIR/results/QC/\"\n",
    "\n",
    "    output_GGIR = pd.read_csv(output_GGIR_path + \"part4_nightsummary_sleep_full.csv\")\n",
    "\n",
    "    tst[sub] = output_GGIR[\"SleepDurationInSpt\"].values[0]\n",
    "\n",
    "    waso[sub] = output_GGIR[\"WASO\"].values[0]\n",
    "\n",
    "    SE[sub] = tst[sub] / (tst[sub] + waso[sub])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove subjects with tst < 5h\n",
    "tst = pd.DataFrame(tst, index = [\"TST\"]).T\n",
    "tst_df = tst[tst[\"TST\"] > 5]\n",
    "\n",
    "waso = pd.DataFrame(waso, index = [\"WASO\"]).T\n",
    "waso_df = waso[tst[\"TST\"] > 5]\n",
    "\n",
    "SE = pd.DataFrame(SE, index = [\"SE\"]).T\n",
    "SE_df = SE[tst[\"TST\"] > 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot for each subject. 3 separate subplot for each boxplot\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(10, 6))\n",
    "\n",
    "sns.boxplot(data = tst_df, ax = axs[0], color = sns.color_palette(\"Set2\")[0], showfliers = False)\n",
    "sns.stripplot(data = tst_df, ax = axs[0], color = \"black\", size = 8, alpha = 0.5)\n",
    "axs[0].set_title(\"Total Sleep Time\")\n",
    "axs[0].set_ylabel(\"TST (hours)\")\n",
    "\n",
    "sns.boxplot(data = waso_df, ax = axs[1], color = sns.color_palette(\"Set2\")[0], showfliers = False)\n",
    "sns.stripplot(data = waso_df, ax = axs[1], color = \"black\", size = 8, alpha = 0.5)\n",
    "axs[1].set_title(\"Wake After Sleep Onset\")\n",
    "axs[1].set_ylabel(\"WASO (hours)\")\n",
    "\n",
    "sns.boxplot(data = SE_df, ax = axs[2], color = sns.color_palette(\"Set2\")[0], showfliers = False)\n",
    "sns.stripplot(data = SE_df, ax = axs[2], color = \"black\", size = 8, alpha = 0.5)\n",
    "axs[2].set_title(\"Sleep Efficiency\")\n",
    "axs[2].set_ylabel(\"SE (%)\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movement algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neurokit2 as nk\n",
    "\n",
    "def hl_envelopes_idx(s, dmin=1, dmax=1, split=False):\n",
    "    \"\"\"\n",
    "    Compute high and low envelopes of a signal s\n",
    "    Parameters\n",
    "    ----------\n",
    "    s: 1d-array, data signal from which to extract high and low envelopes\n",
    "    dmin, dmax: int, optional, size of chunks, use this if the size of the input signal is too big\n",
    "    split: bool, optional, if True, split the signal in half along its mean, might help to generate the envelope in some cases\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    lmin,lmax : high/low envelope idx of input signal s\n",
    "    \"\"\"\n",
    "\n",
    "    # locals min      \n",
    "    lmin = (np.diff(np.sign(np.diff(s))) > 0).nonzero()[0] + 1 \n",
    "    # locals max\n",
    "    lmax = (np.diff(np.sign(np.diff(s))) < 0).nonzero()[0] + 1 \n",
    "    \n",
    "    if split:\n",
    "        # s_mid is zero if s centered around x-axis or more generally mean of signal\n",
    "        s_mid = np.mean(s) \n",
    "        # pre-sorting of locals min based on relative position with respect to s_mid \n",
    "        lmin = lmin[s[lmin]<s_mid]\n",
    "        # pre-sorting of local max based on relative position with respect to s_mid \n",
    "        lmax = lmax[s[lmax]>s_mid]\n",
    "\n",
    "    # global min of dmin-chunks of locals min \n",
    "    lmin = lmin[[i+np.argmin(s[lmin[i:i+dmin]]) for i in range(0,len(lmin),dmin)]]\n",
    "    # global max of dmax-chunks of locals max \n",
    "    lmax = lmax[[i+np.argmax(s[lmax[i:i+dmax]]) for i in range(0,len(lmax),dmax)]]\n",
    "    \n",
    "    return lmin,lmax\n",
    "\n",
    "def compute_envelope(acc, resample = True):\n",
    "    \"\"\"\n",
    "    Compute the envelope of the acceleration signal\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    acc : pd.Series\n",
    "        Band-pass filtered accelerometer signal magnitude vector\n",
    "    resample : bool, optional\n",
    "        If True, resample the envelope to the original size of the signal\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    env_diff : pd.Series\n",
    "        Envelope difference of the acceleration signal\n",
    "    \"\"\"\n",
    "\n",
    "    lmin, lmax = hl_envelopes_idx(acc.values, dmin = 10, dmax = 10)\n",
    "\n",
    "    # adjust shapes\n",
    "    if len(lmin) > len(lmax):\n",
    "        lmin = lmin[:-1]\n",
    "    if len(lmax) > len(lmin):\n",
    "        lmax = lmax[1:]\n",
    "        \n",
    "    upper_envelope = acc.values[lmax]\n",
    "    lower_envelope = acc.values[lmin]\n",
    "                                \n",
    "    if resample:\n",
    "        upper_envelope_res = np.interp(np.arange(len(acc)), lmax, upper_envelope)\n",
    "        lower_envelope_res = np.interp(np.arange(len(acc)), lmin, lower_envelope)\n",
    "        env_diff = pd.Series(upper_envelope_res - lower_envelope_res, index = acc.index)\n",
    "    else:\n",
    "        env_diff = pd.Series(upper_envelope - lower_envelope, index = acc.index[lmax])\n",
    "\n",
    "    return env_diff\n",
    "\n",
    "def detect_bursts(acc, envelope = True, resample_envelope = True, alfa = None):\n",
    "    \"\"\"\n",
    "    Detect bursts in acceleration signal\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    acc : pd.Series\n",
    "        Band-pass filtered accelerometer signal magnitude vector\n",
    "    envelope : bool, optional\n",
    "        If True, detect bursts based on the envelope of the signal\n",
    "        If False, detect bursts based on the std of the signal\n",
    "    resample_envelope : bool, optional\n",
    "        If True, resample the envelope to the original size of the signal\n",
    "    alfa : float, optional\n",
    "        Threshold for detecting bursts\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bursts : pd.Series\n",
    "        pd.DataFrame with burst start times, end times, duration, peak-to-peak amplitude, and AUC\n",
    "    \"\"\"\n",
    "\n",
    "    # band-pass filter the signal\n",
    "    acc = pd.Series(nk.signal_filter(acc.values, sampling_rate = 100, lowcut=0.1, highcut=10, method='butterworth', order=8), index = acc.index)\n",
    "\n",
    "    if envelope:\n",
    "        env_diff = compute_envelope(acc, resample = resample_envelope)\n",
    "        th = alfa\n",
    "    else:\n",
    "        std_acc = acc.resample(\"1 s\").std()\n",
    "        std_acc.index.round(\"1 s\")\n",
    "        th = np.percentile(std_acc, 10) * alfa\n",
    "        env_diff = std_acc\n",
    "\n",
    "    bursts1 = (env_diff > th).astype(int)\n",
    "    start_burst = bursts1.where(bursts1.diff()==1).dropna()\n",
    "    end_burst = bursts1.where(bursts1.diff()==-1).dropna()\n",
    "    if bursts1.iloc[0] == 1:\n",
    "            start_burst = pd.concat([pd.Series(0, index = [bursts1.index[0]]), start_burst])\n",
    "    if bursts1.iloc[-1] == 1:\n",
    "        end_burst = pd.concat([end_burst, pd.Series(0, index = [bursts1.index[-1]])])\n",
    "    bursts_df = pd.DataFrame({\"duration\": end_burst.index - start_burst.index}, index = start_burst.index)\n",
    "\n",
    "    start = bursts_df.index\n",
    "    end = pd.to_datetime((bursts_df.index + bursts_df[\"duration\"]).values)\n",
    "\n",
    "    end = end.to_series().reset_index(drop = True)\n",
    "    start = start.to_series().reset_index(drop = True)\n",
    "\n",
    "    duration_between_bursts = (start.iloc[1:].values - end.iloc[:-1].values)\n",
    "\n",
    "    # If two bursts are too close to each other (5s), consider them as one burst\n",
    "    for i in range(len(start)-1):\n",
    "        if duration_between_bursts[i] < pd.Timedelta(\"5 s\"):\n",
    "            end[i] = np.nan\n",
    "            start[i+1] = np.nan\n",
    "    end.dropna(inplace = True)\n",
    "    start.dropna(inplace = True)\n",
    "\n",
    "    # extract amplitude of the bursts\n",
    "    bursts = pd.DataFrame({\"start\": start.reset_index(drop = True), \"end\": end.reset_index(drop = True)})\n",
    "    p2p = []\n",
    "    auc = []\n",
    "    for i in range(len(bursts)):\n",
    "        # peak-to-peak amplitude of bp acceleration\n",
    "        p2p.append(acc.loc[bursts[\"start\"].iloc[i]:bursts[\"end\"].iloc[i]].max() - acc.loc[bursts[\"start\"].iloc[i]:bursts[\"end\"].iloc[i]].min())\n",
    "        # AUC of env_diff\n",
    "        auc.append(np.trapz(env_diff.loc[bursts[\"start\"].iloc[i]:bursts[\"end\"].iloc[i]]))\n",
    "    bursts[\"duration\"] = bursts[\"end\"] - bursts[\"start\"]\n",
    "    bursts[\"peak-to-peak\"] = p2p\n",
    "    bursts[\"AUC\"] = auc\n",
    "    return bursts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_acc_norm(acc):\n",
    "    # acc_norm = np.sqrt(np.sum(acc**2, axis=1))\n",
    "    acc_norm = np.linalg.norm(acc, axis=1)\n",
    "    return acc_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f7db65819c0>]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TH_WRIST = 20\n",
    "\n",
    "# bursts_lw = detect_bursts(lw_df, resample_envelope = True, alfa = TH_WRIST)\n",
    "\n",
    "acc = pd.read_csv(HOM_path + \"/10_AC/input_GGIR/acc.csv\", index_col = 0)\n",
    "\n",
    "acc_norm  = compute_acc_norm(acc)\n",
    "acc_norm = pd.Series(acc_norm, index = pd.to_datetime(acc.index))\n",
    "acc_norm.head()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(acc_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bursts = {sub: [] for sub in sub_names}\n",
    "ampl_bursts = {sub: [] for sub in sub_names}\n",
    "dur_bursts = {sub: [] for sub in sub_names}\n",
    "\n",
    "TH_WRIST = 20\n",
    "\n",
    "for i, sub in enumerate(sub_names):\n",
    "\n",
    "    acc = pd.read_csv(HOM_path + \"/\" + sub + \"/input_GGIR/acc.csv\", index_col = 0)\n",
    "\n",
    "    acc_norm  = compute_acc_norm(acc)\n",
    "    acc_norm = pd.Series(acc_norm, index = pd.to_datetime(acc.index))\n",
    "\n",
    "    # segment ACC norm during sleep \n",
    "    output_GGIR_path = \"/Volumes/Untitled/Hom3_CinC/CinC/Data_24hPulses/mod_newQuality/\" + sub + \"/output_GGIR/output_input_GGIR/results/QC/\"\n",
    "\n",
    "    output_GGIR = pd.read_csv(output_GGIR_path + \"part4_nightsummary_sleep_full.csv\")\n",
    "\n",
    "    if output_GGIR[\"sleeponset_ts\"].iloc[0][0] == '0':\n",
    "        sleep_onset = pd.to_datetime(str(acc_norm.index[0].date()+ pd.Timedelta(\"1d\")) + \" \" + output_GGIR[\"sleeponset_ts\"].iloc[0])\n",
    "    else: \n",
    "        sleep_onset = pd.to_datetime(str(acc_norm.index[0].date())  + \" \" + output_GGIR[\"sleeponset_ts\"].iloc[0])\n",
    "\n",
    "    wake_onset = pd.to_datetime(str(acc_norm.index[0].date() + pd.Timedelta(\"1d\")) + \" \" + output_GGIR[\"wakeup_ts\"].iloc[0])\n",
    "\n",
    "    acc_norm_night = acc_norm.loc[sleep_onset:wake_onset] * 1000\n",
    "\n",
    "    # detect bursts\n",
    "    bursts = detect_bursts(acc_norm_night, resample_envelope = True, alfa = TH_WRIST)\n",
    "\n",
    "    n_bursts[sub].append(len(bursts))\n",
    "    ampl_bursts[sub].append(bursts[\"AUC\"].sum() / 1000)\n",
    "    dur_bursts[sub].append(bursts[\"duration\"].sum().total_seconds()/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((39, 1), (39, 1), (39, 1))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_bursts_df = pd.DataFrame(n_bursts, index = [\"n_bursts\"]).T\n",
    "ampl_bursts_df = pd.DataFrame(ampl_bursts, index = [\"ampl_bursts\"]).T\n",
    "dur_bursts_df = pd.DataFrame(dur_bursts, index = [\"dur_bursts\"]).T\n",
    "\n",
    "n_bursts_df.shape, ampl_bursts_df.shape, dur_bursts_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bursts_df.to_csv(\"/Volumes/Untitled/Datasets_Serena/n_bursts_HOM.csv\")\n",
    "ampl_bursts_df.to_csv(\"/Volumes/Untitled/Datasets_Serena/ampl_bursts_HOM.csv\")\n",
    "dur_bursts_df.to_csv(\"/Volumes/Untitled/Datasets_Serena/dur_bursts_HOM.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_bursts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10_AC</th>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11_EC</th>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12_FN</th>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13_MS</th>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14_AD</th>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       n_bursts\n",
       "10_AC        94\n",
       "11_EC        63\n",
       "12_FN       160\n",
       "13_MS        42\n",
       "14_AD        69"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_bursts_df = pd.read_csv(\"/Volumes/Untitled/Datasets_Serena/n_bursts_HOM.csv\", index_col = 0)\n",
    "ampl_bursts_df = pd.read_csv(\"/Volumes/Untitled/Datasets_Serena/ampl_bursts_HOM.csv\", index_col = 0)\n",
    "dur_bursts_df = pd.read_csv(\"/Volumes/Untitled/Datasets_Serena/dur_bursts_HOM.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot for each subject. 3 separate subplot for each boxplot\n",
    "\n",
    "# remove subjects with tst < 5h\n",
    "n_bursts_df = n_bursts_df[tst[\"TST\"] > 5]\n",
    "ampl_bursts_df = ampl_bursts_df[tst[\"TST\"] > 5]\n",
    "dur_bursts_df = dur_bursts_df[tst[\"TST\"] > 5]\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(10, 6))\n",
    "\n",
    "sns.boxplot(data = n_bursts_df, ax = axs[0], color = sns.color_palette(\"Set2\")[1], showfliers = False)\n",
    "sns.stripplot(data = n_bursts_df, ax = axs[0], color = \"black\", size = 8, alpha = 0.5)\n",
    "axs[0].set_title(\"Number of ACC Bursts\")\n",
    "axs[0].set_ylabel(\"N Bursts\")\n",
    "\n",
    "sns.boxplot(data = ampl_bursts_df, ax = axs[1], color = sns.color_palette(\"Set2\")[1], showfliers = False)\n",
    "sns.stripplot(data = ampl_bursts_df, ax = axs[1], color = \"black\", size = 8, alpha = 0.5)\n",
    "axs[1].set_title(\"Total Amplitude of Bursts\")\n",
    "axs[1].set_ylabel(\"AUC (g)\")\n",
    "\n",
    "sns.boxplot(data = dur_bursts_df, ax = axs[2], color = sns.color_palette(\"Set2\")[1], showfliers = False)\n",
    "sns.stripplot(data = dur_bursts_df, ax = axs[2], color = \"black\", size = 8, alpha = 0.5)\n",
    "axs[2].set_title(\"Total Duration of Bursts\")\n",
    "axs[2].set_ylabel(\"Duration (min)\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean HR day vs night"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load all E4 data (ACC, PPG, TEMP, EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32_CD\n",
      "35_AA\n"
     ]
    }
   ],
   "source": [
    "fs_temp = 4\n",
    "fs_EDA = 4\n",
    "fs_acc = 32\n",
    "fs_ppg = 64\n",
    "for i, sub_ID in enumerate(sub_names):\n",
    "    print(sub_ID)\n",
    "    ppg_path = glob.glob(HOM_path + sub_ID + '/**/BVP.csv', recursive=True)[0]\n",
    "    acc_path = glob.glob(HOM_path + sub_ID + '/**/ACC.csv', recursive=True)[0]\n",
    "    temp_path = glob.glob(HOM_path + sub_ID + '/**/TEMP.csv', recursive=True)[0]\n",
    "\n",
    "    ###### ACC ######\n",
    "    acc_data = pd.read_csv(acc_path, header = None)\n",
    "    acc_start = acc_data.iloc[0,0] # start time in unix\n",
    "    acc = acc_data.drop([0,1]).reset_index(drop=True) / 64 # convert to g\n",
    "    acc.index = pd.date_range(start=pd.to_datetime(acc_start, unit='s'), periods = len(acc), freq = '0.03125s') # timestamps as index\n",
    "    acc.columns = ['x', 'y', 'z']\n",
    "\n",
    "    ###### PPG ######\n",
    "    ppg_data = pd.read_csv(ppg_path, header = None)\n",
    "    ppg_start = ppg_data.iloc[0,0]\n",
    "    ppg = ppg_data.iloc[2:].reset_index(drop=True)\n",
    "    ppg.index = pd.date_range(start=pd.to_datetime(ppg_start, unit='s'), periods = len(ppg), freq = '0.015625s')\n",
    "    ppg.columns = ['ppg']\n",
    "\n",
    "    ###### EDA ######\n",
    "    eda_path = glob.glob(HOM_path + sub_ID + '/**/EDA.csv', recursive=True)[0]\n",
    "    eda_data = pd.read_csv(eda_path, header = None)\n",
    "    eda_start = eda_data.iloc[0,0]\n",
    "    eda = eda_data.iloc[2:].reset_index(drop=True)\n",
    "    eda.index = pd.date_range(start=pd.to_datetime(eda_start, unit='s'), periods = len(eda), freq = '0.25s')\n",
    "\n",
    "    ###### TEMP ######\n",
    "    temp_data = pd.read_csv(temp_path, header = None)\n",
    "    temp_start = temp_data.iloc[0,0] \n",
    "    temp = temp_data.iloc[2:].reset_index(drop=True)\n",
    "    temp.index = pd.date_range(start=pd.to_datetime(temp_start, unit='s'), periods = len(temp), freq = '0.25s')\n",
    "    temp.columns = ['temp']\n",
    "\n",
    "    if sub_ID == \"2_SL\":\n",
    "        acc = acc.loc[acc.index[-1] - pd.Timedelta(hours = 16):]\n",
    "        ppg = ppg.loc[ppg.index[-1] - pd.Timedelta(hours = 16):]\n",
    "        eda = eda.loc[eda.index[-1] - pd.Timedelta(hours = 16):]\n",
    "        temp = temp.loc[temp.index[-1] - pd.Timedelta(hours = 16):]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    # plt.plot(acc)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(ppg)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(eda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LookOfLife"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "LoL_path = \"/Users/marcellosicbaldi/Library/CloudStorage/OneDrive-AlmaMaterStudiorumUniversitaÌ€diBologna/Serena_Marcello/SleepAnalysis/Datasets/LookOfLife/\"\n",
    "sub_names = sorted([f for f in os.listdir(LoL_path) if not f.startswith('.')])\n",
    "len(sub_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an input_GGIR and an output_GGIR folder for each subject\n",
    "for i, sub_ID in enumerate(sub_names): \n",
    "    days = [d for d in os.listdir(LoL_path + sub_ID) if not d.startswith('.')]\n",
    "    for day in days:\n",
    "        input_GGIR = LoL_path + sub_ID + \"/\" + day + \"/input_GGIR/\"\n",
    "        output_GGIR = LoL_path + sub_ID + \"/\" + day + \"/output_GGIR/\"\n",
    "        if not os.path.exists(input_GGIR):\n",
    "            os.makedirs(input_GGIR)\n",
    "        if not os.path.exists(output_GGIR):\n",
    "            os.makedirs(output_GGIR)\n",
    "        print(f\"{i+1}/{len(sub_names)} - {sub_ID} done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save acc data for GGIR and detect non-wear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_acc = 32\n",
    "for i, sub_ID in enumerate(sub_names[38:]):\n",
    "\n",
    "    print(sub_ID)\n",
    "    days = [d for d in os.listdir(LoL_path + sub_ID) if not d.startswith('.')]\n",
    "    \n",
    "    for day in days:\n",
    "\n",
    "        print(day)\n",
    "        acc_path = glob.glob(LoL_path + sub_ID + '/' + day + '/ACC.csv', recursive=True)[0]\n",
    "        temp_path = glob.glob(LoL_path + sub_ID + '/' + day + '/TEMP.csv', recursive=True)[0]\n",
    "\n",
    "        acc_data = pd.read_csv(acc_path, header = None)\n",
    "        acc_start = acc_data.iloc[0,0] # start time in unix\n",
    "        acc = acc_data.drop([0,1]).reset_index(drop=True) / 64 # convert to g\n",
    "        acc.index = pd.date_range(start=pd.to_datetime(acc_start, unit='s'), periods = len(acc), freq = '0.03125S') # timestamps as index\n",
    "        acc.columns = ['x', 'y', 'z']\n",
    "        # print recording duration\n",
    "        print(f\"Duration: {acc.index[-1] - acc.index[0]}\")\n",
    "        # Save for GGIR\n",
    "        acc.to_csv(LoL_path + sub_ID + '/' + day + '/input_GGIR/acc.csv', index = True, header = True)\n",
    "\n",
    "        ###### TEMP ######\n",
    "        temp_data = pd.read_csv(temp_path, header = None)\n",
    "        temp_start = temp_data.iloc[0,0] \n",
    "        temp = temp_data.iloc[2:].reset_index(drop=True)\n",
    "        temp.index = pd.date_range(start=pd.to_datetime(temp_start, unit='s'), periods = len(temp), freq = '0.25S')\n",
    "        temp.columns = ['temp']\n",
    "\n",
    "        start_stop_nw, _ = nimbaldetach(acc['x'], acc['y'], acc['z'], temp[\"temp\"], accel_freq=32, temperature_freq=4, quiet=True)\n",
    "        start_stop_nw.to_csv(LoL_path + sub_ID + '/' + day + '/input_GGIR/nonwear.csv')\n",
    "\n",
    "        if len(start_stop_nw) > 0:\n",
    "            print(\"Non-wear detected!\")\n",
    "            print(start_stop_nw)\n",
    "            plt.figure(figsize=(10, 6)) \n",
    "            plt.plot(acc[\"x\"].values, label=\"x\")\n",
    "            plt.plot(acc[\"y\"].values, label=\"y\")\n",
    "            plt.plot(acc[\"z\"].values, label=\"z\")\n",
    "            plt.title(sub_ID, fontsize=20)\n",
    "            for _, row in start_stop_nw.iterrows():\n",
    "                start = row[\"Start Datapoint\"]\n",
    "                stop = row[\"End Datapoint\"]\n",
    "                plt.axvspan(start, stop, color='red', alpha=0.5)\n",
    "            plt.legend(loc = \"upper right\")\n",
    "            plt.savefig(LoL_path + sub_ID + '/' + day + '/input_GGIR/nonwear.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PAINLESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PAIN_path = \"/Users/marcellosicbaldi/Library/CloudStorage/OneDrive-AlmaMaterStudiorumUniversitaÌ€diBologna/Serena_Marcello/SleepAnalysis/Datasets/PAINLESS/\"\n",
    "PAIN_path = \"/Volumes/Untitled/PAINLESS/\"\n",
    "sub_names = sorted([f for f in os.listdir(PAIN_path) if not f.startswith('.')])\n",
    "len(sub_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an input_GGIR and an output_GGIR folder for each subject\n",
    "days = [\"Day1\", \"Day2\"]\n",
    "for i, sub_ID in enumerate(sub_names): \n",
    "    for day in days:\n",
    "        input_GGIR = PAIN_path + sub_ID + \"/\" + day + \"/input_GGIR/\"\n",
    "        output_GGIR = PAIN_path + sub_ID + \"/\" + day + \"/output_GGIR/\"\n",
    "        if not os.path.exists(input_GGIR):\n",
    "            os.makedirs(input_GGIR)\n",
    "        if not os.path.exists(output_GGIR):\n",
    "            os.makedirs(output_GGIR)\n",
    "        print(f\"{i+1}/{len(sub_names)} - {sub_ID} done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save acc data for GGIR and detect non-wear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "005\n",
      "Day1\n",
      "Duration: 0 days 23:56:39.156250\n",
      "Day2\n",
      "Duration: 0 days 23:30:11.031250\n",
      "Non-wear detected!\n",
      "   Start Datapoint  End Datapoint\n",
      "1           967184         983256\n",
      "2          1128904        1164136\n",
      "006\n",
      "Day1\n",
      "Duration: 0 days 23:01:55.468750\n",
      "Day2\n",
      "Duration: 0 days 23:59:27.343750\n",
      "007\n",
      "Day1\n",
      "Duration: 0 days 23:55:30.531250\n",
      "Non-wear detected!\n",
      "   Start Datapoint  End Datapoint\n",
      "1          2460872        2509072\n",
      "Day2\n",
      "Duration: 0 days 23:48:29.406250\n",
      "Non-wear detected!\n",
      "   Start Datapoint  End Datapoint\n",
      "1          2312496        2336704\n",
      "008\n",
      "Day1\n",
      "Duration: 0 days 22:36:48.906250\n",
      "Non-wear detected!\n",
      "   Start Datapoint  End Datapoint\n",
      "1          1088320        1099408\n",
      "Day2\n",
      "Duration: 1 days 00:04:16.656250\n",
      "Non-wear detected!\n",
      "   Start Datapoint  End Datapoint\n",
      "1          1051656        1071472\n",
      "009\n",
      "Day1\n",
      "Duration: 0 days 23:15:33.156250\n",
      "Day2\n",
      "Duration: 1 days 00:16:05.218750\n",
      "Non-wear detected!\n",
      "   Start Datapoint  End Datapoint\n",
      "1          2664856        2694256\n",
      "010\n",
      "Day1\n",
      "Duration: 0 days 23:02:44.218750\n",
      "Day2\n",
      "Duration: 1 days 00:24:08.781250\n",
      "011\n",
      "Day1\n",
      "Duration: 0 days 23:11:51.156250\n",
      "Day2\n",
      "Duration: 1 days 00:25:05.406250\n",
      "Non-wear detected!\n",
      "   Start Datapoint  End Datapoint\n",
      "1          2214648        2251496\n",
      "012\n",
      "Day1\n",
      "Duration: 0 days 22:59:49.468750\n",
      "Non-wear detected!\n",
      "   Start Datapoint  End Datapoint\n",
      "1          1025640        1043296\n",
      "Day2\n",
      "Duration: 1 days 00:32:08.406250\n",
      "013\n",
      "Day1\n",
      "Duration: 0 days 22:42:59.593750\n",
      "Day2\n",
      "Duration: 1 days 00:10:17.031250\n",
      "014\n",
      "Day1\n",
      "Duration: 0 days 23:04:41.781250\n",
      "Day2\n",
      "Duration: 1 days 00:01:32.781250\n",
      "Non-wear detected!\n",
      "   Start Datapoint  End Datapoint\n",
      "1          2541280        2668064\n",
      "015\n",
      "Day1\n",
      "Duration: 0 days 22:41:46.468750\n",
      "Day2\n",
      "Duration: 1 days 00:09:34.468750\n",
      "016\n",
      "Day1\n",
      "Duration: 0 days 23:14:03.718750\n",
      "Non-wear detected!\n",
      "   Start Datapoint  End Datapoint\n",
      "1           546088         560312\n",
      "Day2\n",
      "Duration: 0 days 22:59:59.968750\n",
      "017\n",
      "Day1\n",
      "Duration: 0 days 22:14:41.218750\n",
      "Non-wear detected!\n",
      "   Start Datapoint  End Datapoint\n",
      "1           681096         715400\n",
      "Day2\n",
      "Duration: 0 days 23:47:34.468750\n",
      "Non-wear detected!\n",
      "   Start Datapoint  End Datapoint\n",
      "1           746016         787840\n",
      "018\n",
      "Day1\n",
      "Duration: 0 days 23:00:20.968750\n",
      "Day2\n",
      "Duration: 0 days 23:13:12.531250\n"
     ]
    }
   ],
   "source": [
    "fs_acc = 32\n",
    "\n",
    "days = [\"Day1\", \"Day2\"]\n",
    "\n",
    "for i, sub_ID in enumerate(sub_names[4:]):\n",
    "\n",
    "    print(sub_ID)\n",
    "    \n",
    "    for day in days:\n",
    "\n",
    "        print(day)\n",
    "        acc_path = glob.glob(PAIN_path + sub_ID + '/' + day + '/ACC.csv', recursive=True)[0]\n",
    "        temp_path = glob.glob(PAIN_path + sub_ID + '/' + day + '/TEMP.csv', recursive=True)[0]\n",
    "\n",
    "        acc_data = pd.read_csv(acc_path, header = None)\n",
    "        acc_start = acc_data.iloc[0,0] # start time in unix\n",
    "        acc = acc_data.drop([0,1]).reset_index(drop=True) / 64 # convert to g\n",
    "        acc.index = pd.date_range(start=pd.to_datetime(acc_start, unit='s'), periods = len(acc), freq = '0.03125S') # timestamps as index\n",
    "        acc.columns = ['x', 'y', 'z']\n",
    "        # print recording duration\n",
    "        print(f\"Duration: {acc.index[-1] - acc.index[0]}\")\n",
    "        # Save for GGIR\n",
    "        acc.to_csv(PAIN_path + sub_ID + '/' + day + '/input_GGIR/acc.csv', index = True, header = True)\n",
    "\n",
    "        ###### TEMP ######\n",
    "        temp_data = pd.read_csv(temp_path, header = None)\n",
    "        temp_start = temp_data.iloc[0,0] \n",
    "        temp = temp_data.iloc[2:].reset_index(drop=True)\n",
    "        temp.index = pd.date_range(start=pd.to_datetime(temp_start, unit='s'), periods = len(temp), freq = '0.25S')\n",
    "        temp.columns = ['temp']\n",
    "\n",
    "        start_stop_nw, _ = nimbaldetach(acc['x'], acc['y'], acc['z'], temp[\"temp\"], accel_freq=32, temperature_freq=4, quiet=True)\n",
    "        start_stop_nw.to_csv(PAIN_path + sub_ID + '/' + day + '/input_GGIR/nonwear.csv')\n",
    "\n",
    "        if len(start_stop_nw) > 0:\n",
    "            print(\"Non-wear detected!\")\n",
    "            print(start_stop_nw)\n",
    "            plt.figure(figsize=(10, 6)) \n",
    "            plt.plot(acc[\"x\"].values, label=\"x\")\n",
    "            plt.plot(acc[\"y\"].values, label=\"y\")\n",
    "            plt.plot(acc[\"z\"].values, label=\"z\")\n",
    "            plt.title(sub_ID, fontsize=20)\n",
    "            for _, row in start_stop_nw.iterrows():\n",
    "                start = row[\"Start Datapoint\"]\n",
    "                stop = row[\"End Datapoint\"]\n",
    "                plt.axvspan(start, stop, color='red', alpha=0.5)\n",
    "            plt.legend(loc = \"upper right\")\n",
    "            plt.savefig(PAIN_path + sub_ID + '/' + day + '/input_GGIR/nonwear.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ppgenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
